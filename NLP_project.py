from __future__ import division, unicode_literals import codecsfrom bs4 import BeautifulSoupimport osimport pandas as pdimport substringimport nltkfrom nltk.corpus import stopwordsfrom nltk.tokenize import word_tokenizeimport re'''In this part of the program I am parsing through the files adn data I collected toput the name of each .html file in a data frame, so I can parse their paths to get another data frame with the actual raw data. '''paths = pd.DataFrame(columns=['Paths'])for subdir, dirs, files in os.walk(r'/Users/aragaom/Desktop/NLP project/AAA 5 Correlation Download Oct 19, 2021 1025 AM'):    for filename in files:        filepath = subdir + os.sep + filename        if filepath.endswith(".html"):            paths=paths.append({'Paths': filepath},ignore_index=True)            # print (filepath)        continue    submissions = pd.DataFrame(columns=['AAAs'])for path in paths['Paths']:    f=codecs.open(path, 'r', 'utf-8')    document= BeautifulSoup(f.read()).get_text()    submissions = submissions.append({'AAAs': document},ignore_index=True)    # print (document)#print(df_for_texts.iloc[89]['AAAs'])'''Here the NLP should start. '''stop_words = set(stopwords.words('english'))'''here we are tokenizing the words in which one of the texts submitted, I need to undertanwhat tokenizers really are. '''# tokenization is only one of the methods of data processing for nlp# how does the lambda function works?|submissions_tokenized = pd.DataFrame(columns=['tokenized'])# submissions['tokenized sentences'] = submissions.apply(lambda row: nltk.word_tokenize(row['AAAs']), axis=1)for submission in submissions['AAAs']:    cleaned = re.sub(r'[^(a-zA-Z)\s]','', submission)    word_tokens = word_tokenize(cleaned)    submissions_tokenized = submissions_tokenized.append({'tokenized': [word for word in word_tokens if not word in stopwords.words()]},ignore_index=True)    # filtered_sentence = [w for w in word_tokens if not w in stop_words]# print(filtered_sentence)