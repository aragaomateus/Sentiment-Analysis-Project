from __future__ import division, unicode_literals import codecsfrom bs4 import BeautifulSoupimport osimport pandas as pdimport substringimport nltkfrom nltk.corpus import stopwordsfrom nltk.tokenize import word_tokenizeimport re'''In this part of the program I am parsing through the files adn data I collected toput the name of each .html file in a data frame, so I can parse their paths to get another data frame with the actual raw data. '''paths = pd.DataFrame(columns=['Paths'])for subdir, dirs, files in os.walk(r'/Users/aragaom/Desktop/Sentiment-Analysis-Project/AAA 5 Correlation Download Oct 19, 2021 1025 AM'):    for filename in files:        filepath = subdir + os.sep + filename        if filepath.endswith(".html"):            paths=paths.append({'Paths': filepath},ignore_index=True)            # print (filepath)        continue    submissions = pd.DataFrame(columns=['AAAs'])for path in paths['Paths']:    f=codecs.open(path, 'r', 'utf-8')    document= BeautifulSoup(f.read()).get_text()    submissions = submissions.append({'AAAs': document},ignore_index=True)    # print (document)#print(df_for_texts.iloc[89]['AAAs'])'''Here the NLP should start. '''stop_words = set(stopwords.words('english'))'''here we are tokenizing the words in which one of the texts submitted, I need to understandwhat tokenizers really are. '''# tokenization is only one of the methods of data processing for nlp# how does the lambda function works?|'''Finishing the data processing removing commas and other punctuations, spaces and stop words. Now the data frame is ready with the '''submissions_tokenized = pd.DataFrame(columns=['tokenized'])# submissions['tokenized sentences'] = submissions.apply(lambda row: nltk.word_tokenize(row['AAAs']), axis=1)for submission in submissions['AAAs']:    cleaned = re.sub(r'[^(a-zA-Z)\s]','', submission)    word_tokens = word_tokenize(cleaned)    submissions_tokenized = submissions_tokenized.append({'tokenized': [word for word in word_tokens if not word in stopwords.words()]},ignore_index=True)#%%'''using wordnet from nltk package, dont know why.'''from nltk.corpus import wordnetfor i in submissions_tokenized['tokenized'][2]:    print(i)    syns = wordnet.synsets(i)    print(syns)    #%%from nltk.probability import FreqDistfor i in range(len(submissions_tokenized['tokenized'])):    fdist = FreqDist(submissions_tokenized['tokenized'][i])    print(fdist.most_common(5))    import matplotlib.pyplot as plt    fdist.plot(30,cumulative=False)    plt.show()    #%%print('mateus')